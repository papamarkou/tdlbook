# Motivation

Combinatorial complex neural networks (CCNNs), as presented in this work, generalize graph neural networks (GNNs) and their higher-order analogues via topological constructions. In this section, we provide the motivation for the use of topological constructions in machine learning, and specifically in deep learning from three angles. First, *data modeling*: how topological abstraction can help us to reason and compute with various data types supported on topological spaces. Second, *utility*: how accounting for topology improves performance. Third, *unification*: how topology can be used to synthesize and abstract disparate concepts.

## Modeling and learning from data on topological spaces

In the context of machine learning, the domain on which the data is supported is typically a (linear) vector space. This underlying vector space facilitates many computations and is often implicitly assumed. However, as has been recognized within geometric deep learning, it is often essential to consider data supported on different domains. Indeed, explicitly considering and modeling the domain on which the data is supported can be crucial for various reasons.

First, different domains can have distinct characteristics and properties that require different types of deep learning models to effectively process and analyze the data. For example, a graph domain may require a graph convolutional network [@kipf2016semi] to account for the non-Euclidean structure, while a point cloud domain may require a PointNet-like architecture [@qi2017pointnet; @zaheer2017deep] to handle unordered sets of points.

Second, the specific data within each domain can vary widely in terms of size, complexity and noise. By understanding specific data properties within a given domain, researchers can develop models that are tailored to those particular properties. For example, a model designed for point clouds with high levels of noise may incorporate techniques such as outlier removal or local feature aggregation.

Third, accurately distinguishing between domain and data is important for developing models that generalize well to new, unseen data. By identifying the underlying structure of the domain, researchers can develop models that are robust to variations in the data while still being able to capture relevant geometric features. To summarize, by considering both the domain and data together, researchers can develop models that are better suited to handle the specific challenges and complexities of different geometric learning tasks.

**Different perspectives on data: domains and relations**. When referring to topological data or topological data analysis in the literature, there are different views on what the actual data consists of and what it is supposed to model. Here, we follow [@bick2021higher] and distinguish between different types of data and goals of our learning procedures, even though the distinction between these different types of data can sometimes be more blurry than what is presented here.

*Relational data*. Relational data describes the relations between different entities or objects. 
This fundamental concept can manifest in various ways, such as connections between users in social networks, where friend relationships or follower-ship serve as examples of such relations. Traditionally, these relations are understood through graphs, with vertices and edges representing entities and their *pairwise connections*, respectively. Stated differently, the edges within the graph are considered to be units of measurement; i.e., every piece of relational data involves two vertices.

However, many real-world phenomena naturally involve complex, multi-party interactions, where more than two entities interact with each other in intricate ways. For instance, groups of individuals within a society, sets of co-authors, and genes or proteins that interact with each other are all examples of such higher-order relations. Such dependencies, which go beyond pairwise interactions, can be modeled by *higher-order relations* and aptly abstracted as hypergraphs, cell complexes, or CCs. Clearly, topological ideas can play an important role for understanding this data, and crucially enable us to move from graphs modeling pairwise relations to richer representations.

*Data as an instance of a domain*. A related conceptualization of what our observed data is supposed to represent is adopted in TDA. Namely, all of observed data is supposed to correspond to a noisy instance of a topological object itself; i.e., we aim to learn the `topological shape' of the data. Note that in this context, we typically do not directly observe relational data, but instead we *construct relations from observed data*, which we then use to classify the observed dataset. Persistent homology performed on point cloud data is a mainstream example for this viewpoint.

*Data supported on a topological domain*. Topological ideas also play an important role when considering data that is defined on top of a domain such as a graph. This could be particular dynamics, such as epidemic spreading, or it could be any type of other data or signals supported on the vertices (cases of edge-signals on a graph are not considered often, though exceptions exist [@schaub2018denoising; @schaub2021signal]). In the language of *graph signal processing*, a variety of functions or signals can be defined on a graph *domain*, and these are said to be *supported by the domain*. Crucially, the graph itself can be arbitrarily complex but is typically considered to be fixed; the observed data is not relational, but supported on the vertices of a graph.

Moving beyond graphs, more general topological constructions, such as CCs, can support data not just on vertices and edges, but also on other `higher-order' entities, as demonstrated in Figure \@ref(fig:support)(a-b)^[An interactive visualization of (a-b) is provided [here](https://app.vectary.com/p/5uLAflZj6U2kvACv2kk2tN).]. For example, vector fields defined on meshes in computer graphics are often data supported on edges and faces, and can be conveniently modeled as data supported on a higher-order domain [@de2016vector]. Similarly, class-labeled data can be provided on the edges and faces of a given mesh; see Figure \@ref(fig:support)(c) for an example. To process such data it is again relevant to take into account the structure of the underlying topological domain.

```{r support, echo=FALSE, fig.align="center", fig.cap="Data might be supported naturally on higher-order relations. (a): An edge-based vector field. (b): A face-based vector field. Both vector fields in (a) and (b) are defined on a cell complex torus. (c): Class-labeled topological data might naturally be supported on higher-order relations. For instance, mesh segmentation labels for 2-faces are depicted by different colors (blue, green, turquoise, pink, brown) to represent different parts (head, neck, body, legs, tail) of the horse."}
knitr::include_graphics('figures/two_tori_horse.png', dpi=NA)
```

**Modeling and processing data beyond graphs: illustrative examples**. Graphs are well-suited for modeling systems that exhibit pairwise interactions. For instance, particle-based fluid simulations can be effectively represented using graphs as shown in Figure \@ref(fig:cloth)(a), with message passing used to update the physical state of fluid molecules. In this approach, each molecule is represented as a vertex containing its physical state, and the edges connecting them are utilized to calculate their interactions [@sanchez2020learning; @shlomi2020graph].

```{r cloth, echo=FALSE, fig.align="center", fig.cap="Examples of processing data supported on graphs or on higher-order networks. (a): Graphs can be used to model particle interactions in fluid dynamics. Vertices represent particles, whereas particle-to-particle interactions are modeled via message passing among vertices. (b): When modeling springs and self-collision, it is natural to work with edges rather than vertices. This is because the behavior of the cloth is determined by the tension and compression forces acting along the edges, and not only by the position of individual particles. To model the interactions among multiple edges, polygonal faces can be used to represent the local geometry of the cloth. The polygonal faces provide a way to compute higher-order message passing among the edges."}
knitr::include_graphics('figures/cloth.png', dpi=NA)
```

Graphs may not be adequate for modeling more complex systems such as cloth simulation, where state variables are associated with relations like edges or triangular faces rather than vertices. In such cases, higher-order message passing is required to compute and update physical states, as depicted in Figure \@ref(fig:cloth)(b). A similar challenge arises in natural language processing, where language exhibits multiple layers of syntax, semantics and context. Although GNNs may capture basic syntactic and semantic relations between words, more complex relations such as negation, irony or sarcasm may be difficult to represent [@girault2017towards]. Including higher-order and hierarchical relations can provide a better model for these relations, and can enable a more nuanced and accurate understanding of language.
