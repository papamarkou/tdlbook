# Combinatorial complex neural networks

The modelling flexibility of CCs enables the exploration and analysis of a wide spectrum of CC-based neural network architectures. A CC-based neural network can exploit all neighborhood matrices or a subset of them, thus accounting for multi-way interactions among various cells in the CC to solve a learning task. In this section, we introduce the blueprint for TDL by developing the general principles of CC-based TDL models. We utilize our TDL blueprint framework for examining current approaches and offer directives for designing novel models.

The learning tasks in TDL can be broadly classified into three categories: cell classification, complex classification, and cell prediction. See Figure \@ref(fig:tdl-tasks). Our numerical experiments in Chapter \@ref(equivariances-of-ccnns) provide examples on cell and complex classification. In more detail, the learning tasks of the three categories are the following:

- *Cell classification*: the goal is to predict targets for each cell in a complex. To accomplish this, we can utilize a TDL classifier that takes into account the topological neighbors of the target cell and their associated features. An example of cell classification is triangular mesh segmentation, in which the task is to predict the class of each face or edge in a given mesh.
- *Complex classification*: the aim is to predict targets for an entire complex. To achieve this, we can reduce the topology of the complex into a common representation using higher-order cells, such as pooling, and then learn a TDL classifier over the resulting flat vector. An example of complex classification is class prediction for each input mesh.
- *Cell prediction*: the objective is to predict properties of cell-cell interactions in a complex, and, in some cases, to predict whether a cell exists in the complex. This can be achieved by utilizing the topology and associated features of the cells. A relevant example is the prediction of linkages among entities in hyperedges of a hypergraph.

```{r tdl-tasks, echo=FALSE, fig.align="center", fig.cap="Learning on topological spaces can be broadly classified in three main tasks. (1) *Cell classification*: this task predicts targets for individual cells within a complex. An example of this tasks is mesh segmentation where the topological neural network outputs a segmentation label for every face in the input mesh. (2) *Complex classification*: predicting targets for entire complexes involves reducing topology into a common representation. An example of this task is class prediction for input meshes. (3) *Cell prediction*: forecasting properties of cell-cell interactions, sometimes including predicting cell existence, by leveraging topology of the underlying complex and associated features. An example of this task is predicting linkages in hypergraph hyperedges."}
knitr::include_graphics('figures/tasks.png', dpi=NA)
```

Figure \@ref(fig:tdl) outlines our general setup for TDL. Initially, a higher-order domain, represented by a CC, is constructed on a set $S$. A set of neighborhood functions defined on the domain is then selected. The neighborhood functions are usually selected based on the learning problem at hand and they are used to build a topological neural network. To develop our general TDL framework, we introduce *combinatorial complex neural networks (CCNNs)*, an abstract class of neural networks supported on CCs that effectively captures the pipeline of Figure \@ref(fig:tdl). CCNNs can be thought of as a *template* that generalizes many popular architectures, such as convolutional and attention-based neural networks. The abstraction of CCNNs offers many advantages. First, any result that holds for CCNNs is immediately applicable to any particular instance of CCNN architecture. Indeed, the theoretical analysis and results in this paper are applicable to any CC-based neural network as long as it satisfies the CCNN definition. Second, working with a particular parametrization might be cumbersome if the neural network has a complicated architecture. In Section \@ref(building-ccnns-tensor-diagrams), we elaborate on the intricate architectures of
parameterized TDL models. The more abstract high-level representation of CCNNs simplifies the notation and the general purpose of the learning process, thereby making TDL modelling more intuitive to handle.

```{r tdl, echo=FALSE, fig.align="center", fig.cap="A TDL blueprint. (a): A set of abstract entities. (b): A CC $(S, \\mathcal{X}, \\mbox{rk})$ is defined on $S$. (c): For an element $x \\in \\mathcal{X}$, we select a collection of neighborhood functions defined on the CC. (d): We build a neural network on the CC using the neighborhood functions selected in (c). The neural network exploits the neighborhood functions selected in (c) to update the data supported on $x$."}
knitr::include_graphics('figures/tdl_blue_print.png', dpi=NA)
```

```{definition, hoans-definition, name="Combinatorial complex neural networks"}
Let $\mathcal{X}$ be a CC. Let $\mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times  \mathcal{C}^{i_m}$ and $\mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times  \mathcal{C}^{j_n}$ be a Cartesian product of $m$ and $n$ cochain spaces defined on $\mathcal{X}$. A *combinatorial complex neural network (CCNN)* is a function of the form
\begin{equation*}
\mbox{CCNN}: \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times  \mathcal{C}^{i_m} \longrightarrow \mathcal{C}^{j_1}\times\mathcal{C}^{j_2}\times \ldots \times \mathcal{C}^{j_n}.
\end{equation*}
```

Intuitively, a CCNN takes a vector of cochains $(\mathbf{H}_{i_1},\ldots, \mathbf{H}_{i_m})$ as input and returns a vector of cochains $(\mathbf{K}_{j_1},\ldots, \mathbf{K}_{j_n})$ as output. In Section \@ref(building-ccnns-tensor-diagrams), we show how neighborhood functions play a central role in the construction of a general CCNN. Definition \@ref(def:hoans-definition) does not show how a CCNN can be computed in general. Chapters \@ref(message-passing) and \@ref(push-forward-pooling-and-unpooling) formalize the computational workflow in CCNNs.

## Building CCNNs: tensor diagrams

Unlike graphs that involve vertex or edge signals, higher-order networks entail a higher number of signals (see Figure \@ref(fig:cc-cochain)). Thus, constructing a CCNN requires building a non-trivial amount of interacting sub-neural networks. Due to the potentially large number of cochains procssed via a CCNN, we introduce *tensor diagrams*, a diagrammatic notation for that describes a generic computational model supported on a topological domain and describing the flow of various signals supported and processed on this domain.

```{remark}
Diagrammatic notation is common in the geometric topology literature [@hatcher2005algebraic; @turaev2016quantum], and it is typically used to construct functions built from simpler building blocks. See Appendix \@ref(ccnn-architecture-search-and-topological-quantum-field-theories) for further discussion. See also [@roddenberry2021principled] for related constructions on simplicial neural networks.
```

```{definition, tdd, name="Tensor diagram"}
A *tensor diagram* represents a CCNN via a directed graph. The signal on a tensor diagram flows from the *source nodes* to the *target nodes*. The source and target nodes correspond to the domain and codomain of the CCNN.
```

Figure \@ref(fig:td) depicts an example of a tensor diagram. On the left, a CC of dimension three is shown. Consider a 0-cochain $\mathcal{C}^0$, a 1-cochain $\mathcal{C}^1$ and a 2-cochain $\mathcal{C}^2$. The middle figure displays a CCNN that maps a cochain vector in $\mathcal{C}^0 \times \mathcal{C}^1\times \mathcal{C}^2$ to a cochain vector in $\mathcal{C}^0\times\mathcal{C}^1 \times \mathcal{C}^2$. On the right, a tensor diagram representation of the CCNN is shown. We label each edge on the tensor diagram by a cochain map or by its matrix representation. The edge labels on the tensor diagram of Figure \@ref(fig:td) are $A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}$ and $coA_{2,1}$. Thus, the tensor diagram specifies the flow of cochains on the CC.

```{r td, echo=FALSE, fig.align="center", fig.cap="A tensor diagram is a diagrammatic representation of a CCNN that captures the flow of signals on the CCNN."}
knitr::include_graphics('figures/tensor_diagram.png', dpi=NA)
```

The  labels on the arrows of a tensor diagram form a sequence $\mathbf{G}= (G_i)_{i=1}^l$ of cochain maps defined on the underlying CC. In Figure \@ref(fig:td) for example, $\mathbf{G}=(G_i)_{i=1}^5 = (A_{0,1}, B_{0,1}^{T}, A_{1,1}, B_{1,2}, coA_{2,1})$. When a tensor diagram is used to represent a CCNN, we use the notation $\mbox{CCNN}_{\mathbf{G}}$ for the tensor diagram and for its corresponding CCNN. The cochain maps $(G_i)_{i=1}^l$ reflect the structure of the CC and are used to determine the flow of signals on the CC. Any of the neighborhood matrices mentioned in Section \@ref(neighborhood-functions-on-ccs) can be used as cochain maps. The choice of cochain maps depends on the learning task.

Figure \@ref(fig:tensor) visualizes additional examples of tensor diagrams. The *height* of a tensor diagram is the number of edges on a longest path from a source node to a target node. For instance, the heights of the tensor diagrams in Figures \@ref(fig:tensor)(a) and \@ref(fig:tensor)(d) are one and two, respectively. The vertical concatenation of two tensor diagrams represents the composition of their corresponding CCNNs. For example, the tensor diagram in Figure \@ref(fig:tensor)(d) is the vertical concatenation of the tensor diagrams in Figures \@ref(fig:tensor)(c) and (b).

```{r tensor, echo=FALSE, fig.align="center", fig.cap="Examples of tensor diagrams. (a): Tensor diagram of a $\\mbox{CCNN}_{coA_{1,1}}\\colon \\mathcal{C}^1 \\to \\mathcal{C}^1$. (b): Tensor diagram of a $\\mbox{CCNN}_{ \\{B_{1,2}, B_{1,2}^T\\}} \\colon \\mathcal{C}^1 \\times \\mathcal{C}^2 \\to \\mathcal{C}^1 \\times \\mathcal{C}^2$. (c): A merge node that merges three cochains. (d): A tensor diagram generated by vertical concatenation of the tensor diagrams in (c) and (b). The edge label $Id$ denotes the identity matrix."}
knitr::include_graphics('figures/hon_example.png', dpi=NA)
```

If a node in a tensor diagram receives one or more signals, we call it a *merge node*. Mathematically, a merge node is a function $\mathcal{M}_{G_1,\ldots ,G_m}\colon \mathcal{C}^{i_1}\times\mathcal{C}^{i_2}\times \ldots \times \mathcal{C}^{i_m} \to \mathcal{C}^{j}$ given by
\begin{equation}
	(\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m}) \xrightarrow[]{\mathcal{M}} \mathbf{K}_{j}=
	\mathcal{M}_{G_1,\ldots,G_m}(\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m}),
	(\#eq:sum)
\end{equation}
where $G_k \colon C^{i_k}(\mathcal{X})\to C^{j}(\mathcal{X}), k=1,\ldots,m$, are cochain maps. We think of $\mathcal{M}$ as a message-passing function that takes into account the messages outputted by maps $G_1,\ldots,G_m$, which collectively act on a cochain vector $(\mathbf{H}_{i_1},\ldots,\mathbf{H}_{i_m})$, to obtain an updated cochain $\mathbf{K}_{j}$. See Sections \@ref(push-forward-operator-and-merge-node) and \@ref(higher-order-message-passing-neural-networks-are-ccnns) for more details. Figure \@ref(fig:tensor)(c) shows a merge node example.

## Push-forward operator and merge node

We introduce the push-forward operation, a computational scheme that enables sending a cochain supported on $i$-cells to $j$-cells. The push-forward operation is a computational building block used to formalize the definition of the merge nodes given in Equation \@ref(eq:sum), the higher-order message passing introduced in Chapter \@ref(message-passing), and the (un)pooling operations introduced in Section \@ref(push-forward-pooling-and-unpooling).

```{definition, pushing-exact-definition, name="Cochain push-forward"}
Consider a CC $\mathcal{X}$, a cochain map $G\colon\mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})$, and a cochain $\mathbf{H}_i$ in $\mathcal{C}^i(\mathcal{X})$. A *(cochain) push-forward* induced by $G$ is an operator $\mathcal{F}_G \colon \mathcal{C}^i(\mathcal{X})\to \mathcal{C}^j(\mathcal{X})$ defined via
\begin{equation}
\mathbf{H}_i \to \mathbf{K}_j=[ \mathbf{k}_{y^j_1},\ldots,\mathbf{k}_{y^j_{|\mathcal{X}^j|} }] = \mathcal{F}_G(\mathbf{H}_i),
\end{equation}
such that for $k=1,\ldots,|\mathcal{X}^j|$,
\begin{equation}
\mathbf{k}_{y_k^j}= \bigoplus_{x_l^i \in \mathcal{N}_{G^T(y_k^j)}} \alpha_{G} ( \mathbf{ \mathbf{h}_{x_l^i}}),
(\#eq:functional)
\end{equation}
where $\bigoplus$ is a permutation-invariant aggregation function and $\alpha_G$ is a differentiable function.
```

The operator $\mathcal{F}_{G}$ pushes forward an $i$-cochain $\mathbf{H}_i$ supported on $\mathcal{X}^i$ to a $j$-cochain $\mathcal{F}_{G}(\mathbf{H}_i)$ supported on $\mathcal{X}^j$. For every cell $y \in \mathcal{X}^j$, Equation \@ref(eq:functional) constructs the vector $\mathbf{k}_y$ by aggregating all vectors $\mathbf{h}_x$ attached to the neighbors $x \in \mathcal{X}^i$ of $y$ with respect to the neighborhood function $\mathcal{N}_{G^T}$, and by then applying a differentiable function $\alpha_G$ on the set of aggregated vectors $\{ \mathbf{h}_x| x\in \mathcal{N}_{G^T}(y)\}$.

Figure \@ref(fig:push-forward) visualizes two examples of push-forward operators. Example \@ref(exm:non-trainable-pushforward) provides a push-forward function induced by an indicence matrix. The push-forward function in Example \@ref(exm:non-trainable-pushforward) does not contain any parameters, therefore it is not trainable. In Section \@ref(definition-of-combinatorial-complex-convolutional-networks),
we give examples of parameterized push-forward operations, whose parameters can be learnt.

```{r push-forward, echo=FALSE, fig.align="center", fig.cap="Examples of push-forward operators. (a): Let $G_1\\colon \\mathcal{C}^1\\to \\mathcal{C}^2$ be a cochain map. A push-forward $\\mathcal{F}_{G_1}$ induced by $G_1$ takes as input a 1-cochain $\\textbf{H}_{1}$ defined on the edges of the underlying CC $\\mathcal{X}$ and 'pushes-forward' this cochain to a 2-cochain $\\mathbf{K}_2$ defined on $\\mathcal{X}^2$. The cochain $\\mathbf{K}_2$ is formed by aggregating the information in $\\mathbf{H}_1$ using the neighborhood function $\\mathcal{N}_{G_1^T}$. In this case, the neighbors of the 2-rank (blue) cell with respect to $G_1$ are the four (pink) edges on the boundary of this cell. (b): Similarly, $G_2\\colon \\mathcal{C}^0\\to \\mathcal{C}^2$ induces a push-forward map  $\\mathcal{F}_{G_2}\\colon \\mathcal{C}^0\\to \\mathcal{C}^2$ that sends a 0-cochain $\\mathbf{H}_0$ to a 2-cochain $\\mathbf{K}_2$. The cochain $\\mathbf{K}_2$ is defined by aggregating the information in $\\mathbf{H}_0$ using the neighborhood function $\\mathcal{N}_{G_2^T}$."}
knitr::include_graphics('figures/push_forward.png', dpi=NA)
```

```{example, non-trainable-pushforward, name="Push-forward of a CC of dimension two"}
Consider a CC $\mathcal{X}$ of dimension 2. Let $B_{0,2}\colon \mathcal{C}^2 (\mathcal{X})\to \mathcal{C}^0 (\mathcal{X})$ be an incidence matrix. The function $\mathcal{F}^{m}_{B_{0,2}}\colon\mathcal{C}^2 (\mathcal{X})\to \mathcal{C}^0 (\mathcal{X})$ defined by $\mathcal{F}^{m}_{B_{0,2}}(\mathbf{H_{2}})= B_{0,2} (\mathbf{H}_{2})$ is a push-forward induced by $B_{0,2}$. $\mathcal{F}^{m}_{B_{0,2}}$ pushes forward the cochain $\mathbf{H}_{2}\in \mathcal{C}^2$ to cochain $B_{0,2} (\mathbf{H}_{2}) \in \mathcal{C}^0$.
```

In Definition \@ref(def:exact-definition-merge-node), we formulate the notion of merge node using push-forward operators. Figure \@ref(fig:merge-node) visualizes Definition \@ref(def:exact_definition-merge-node) of merge node via a tensor diagram.

```{definition, exact-definition-merge-node, name="Merge node"}
Let $\mathcal{X}$ be a CC. Moreover, let $G_1\colon\mathcal{C}^{i_1}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})$ and $G_2\colon\mathcal{C}^{i_2}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})$ be two cochain maps. Given a cochain vector $(\mathbf{H}_{i_1},\mathbf{H}_{i_2}) \in \mathcal{C}^{i_1}\times \mathcal{C}^{i_2}$, a *merge node* $\mathcal{M}_{G_1,G_2}\colon\mathcal{C}^{i_1} \times \mathcal{C}^{i_2} \to \mathcal{C}^j$ is defined as
\begin{equation}
    \mathcal{M}_{G_1,G_2}(\mathbf{H}_{i_1},\mathbf{H}_{i_2})= \beta\left( \mathcal{F}_{G_1}(\mathbf{H}_{i_1})  \bigotimes \mathcal{F}_{G_2}(\mathbf{H}_{i_2}) \right),
\end{equation}
where $\bigotimes \colon \mathcal{C}^j \times \mathcal{C}^j \to \mathcal{C}^j$ is an aggregation function, $\mathcal{F}_{G_1}$ and $\mathcal{F}_{G_2}$ are push-forward operators induced by $G_1$ and $G_2$, and $\beta$ is an activation function.
```

```{r merge-node, echo=FALSE, fig.align="center", fig.cap="Visualization of the definition of merge node."}
knitr::include_graphics('figures/merge_node_scaled.png', dpi=NA)
```

## The main three tensor operations

Any tensor diagram representation of a CCNN can be built from two elementary operations: the push-forward operator and the merge node. In practice, it is convenient to introduce other operations that facilitate building involved neural network architectures more effectively. For example, one useful operation is the dual operation of the merge node, which we call the split node.

```{definition, exact-definition-split-node, name="Split node"}
Let $\mathcal{X}$ be a CC. Moreover, let $G_1\colon\mathcal{C}^{j}(\mathcal{X})\to\mathcal{C}^{i_1}(\mathcal{X})$ and $G_2\colon\mathcal{C}^{j}(\mathcal{X})\to\mathcal{C}^{i_2}(\mathcal{X})$ be two cochain maps. Given a cochain $\mathbf{H}_{j} \in \mathcal{C}^{j}$, a *split node* $\mathcal{S}_{G_1,G_2}\colon\mathcal{C}^j \to \mathcal{C}^{i_1} \times \mathcal{C}^{i_2}$ is defined as
\begin{equation}
\mathcal{S}_{G_1,G_2}(\mathbf{H}_{j})= \left(  \beta_1(\mathcal{F}_{G_1}(\mathbf{H}_{j})) , \beta_2(\mathcal{F}_{G_2}(\mathbf{H}_{j})) \right),
\end{equation}
where $\mathcal{F}_{G_i}$ is a push-forward operator induced by $G_i$, and $\beta_i$ is an activation function for $i=1, 2$.
```

While it is clear from Definition \@ref(def:exact-definition-split-node) that split nodes are simply tuples of push-forward operations, using split nodes allows us to build neural networks more effectively and intuitively. Definition \@ref(def:elem-opers) puts forward a set of elementary tensor operations, including split nodes, to facilitate the formulation of CCNNs in terms of tensor diagrams.

```{definition, elem-opers, name="Elementary tensor operations"}
We refer collectively to push-forward operations, merge nodes and split nodes as elementary tensor operations.
```

Figure \@ref(fig:split-merge-pushforward) displays tensor diagrams of elementary tensor operations, and Figure \@ref(fig:prior-work) exemplifies how existing topological neural networks are expressed via tensor diagrams based on elementary tensor operations. For example, the simplicial complex net (SCoNe), a Hodge decomposition-based neural network proposed by [@roddenberry2021principled], can be effectively realized in terms of split and merge nodes, as shown in Figure \@ref(fig:prior-work)(a).

```{r split-merge-pushforward, echo=FALSE, fig.align="center", fig.cap="Tensor diagrams of the elementary tensor operations, namely of push-forward operations, merge nodes and split nodes. These three elementary tensor operations are building blocks for constructing tensor diagrams of general CCNNs. A general tensor diagram can be formed using compositions and horizontal concatenations of the three elementary tensor operations. (a): A tensor diagram of a push-forward operation induced by a cochain map $G\\colon\\mathcal{C}^i \\to \\mathcal{C}^j$. (b): A merge node induced by two cochain maps $G_1\\colon\\mathcal{C}^{i_1} \\to \\mathcal{C}^j$ and $G_2\\colon\\mathcal{C}^{i_2} \\to \\mathcal{C}^j$. (c): A split node induced by two cochain maps $G_1\\colon\\mathcal{C}^{j}\\to\\mathcal{C}^{i_1}$ and $G_2\\colon\\mathcal{C}^{j}\\to\\mathcal{C}^{i_2}$. In this illustration, the function $\\Delta \\colon \\mathcal{C}^{j}\\to \\mathcal{C}^{j}\\times \\mathcal{C}^{j}$ is defined as $\\Delta(\\mathbf{H}_j)= (\\mathbf{H}_j,\\mathbf{H}_j)$."}
knitr::include_graphics('figures/split_merge_push.png', dpi=NA)
```

```{r prior-work, echo=FALSE, fig.align="center", fig.cap="Examples of existing neural networks that can be realized in terms of the three elementary tensor operations. Edge labels are dropped to simplify exposition. (a): The simplicial complex net (SCoNe), proposed by [@roddenberry2021principled], can be realized as a composition of a split node that splits an input 1-cochain to three cochains of dimensions zero, one, and two, followed by a merge node that merges these cochains into a 1-cochain. (b): The simplicial neural network (SCN), proposed by [@ebli2020simplicial], can be realized in terms of push-forward operations. (c)--(e): Examples of cell complex neural networks (CXNs); see [@hajijcell]. Note that (e) can be realized in terms of a single merge node that merges the 0 and 2-cochains to a 1-cochain as well as a single split node that splits the 1-cochain to 0- and 2-cochains."}
knitr::include_graphics('figures/prior_work.png', dpi=NA)
```

```{remark}
The elementary tensor operations constitute the only framework needed to define any parameterized topological neural network. Indeed, since tensor diagrams can be built via the three elementary tensor operations, then it suffices to define the push-forward and the merge operators in order to fully define a parameterized class of CCNNs (recall that the split node is completely determined by the push-forward operator). In Sections \@ref(definition-of-combinatorial-complex-convolutional-networks) and \@ref(combinatorial-complex-attention-neural-networks), we build two parameterized classes of CCNNs: the convolutional and attention classes. In both cases, we only define their corresponding parameterized elementary tensor operations. Beyond convolutional and attention versions of CCNNs, the three elementary tensor operations allow us to build arbitrary parameterized tensor diagrams, therefore providing scope to discover novel topological neural network architectures on which our theory remains applicable.
```

An alternative way of constructing CCNNs draws ideas from topological quantum field theory (TQFT). In Appendix \@ref(ccnn-architecture-search-and-topological-quantum-field-theories), we briefly discuss this relationship in more depth.

## Definition of combinatorial complex convolutional networks

One of the fundamental computational requirements for deep learning on higher-order domains is the ability to define and compute convolutional operations. Here, we introduce CCNNs equipped with convolutional operators, which we call *combinatorial complex convolutional neural networks (CCCNNs)*. In particular, we put forward two convolutional operators for CCCNNs: CC-convolutional push-forward operators and CC-convolutional merge nodes.

We demonstrate how CCCNNs can be introduced from the two basic blocks: the push-forward and the merge operations which have been defined abstractly in Section \@ref(push-forward-operator-and-merge-node). In its simplest form, a CC-convolutional push-forward, as conceived in Definition \@ref(def:cc-conv-pushforward), is a generalization of the convolutional graph neural network introduced in [@kipf2016semi].

```{definition, cc-conv-pushforward, name="CC-convolutional push-forward"}
Consider a CC $\mathcal{X}$, a cochain map $G\colon \mathcal{C}^i (\mathcal{X}) \to \mathcal{C}^j(\mathcal{X})$, and a cochain $\mathbf{H}_i \in C^i(\mathcal{X}, \mathbb{R}^{{s}_{in}})$. A \textbf{CC-convolutional push-forward} is a cochain map $\mathcal{F}^{conv}_{G;W} \colon C^i(\mathcal{X}, \mathbb{R}^{{s}_{in}}) \to C^j(\mathcal{X}, \mathbb{R}^{{t}_{out}})$ defined as
\begin{equation}
\mathbf{H}_i \to  \mathbf{K}_j=  G \mathbf{H}_i W ,
(\#eq:cc-conv-push-forward-eq)
\end{equation}
where $W \in \mathbb{R}^{d_{s_{in}}\times d_{s_{out}}}$ are trainable parameters.
```

Having defined the CC-convolutional push-forward, the CC-convolutional merge node (Definition \@ref(def:cc-convolutional)) is a straightforward application of Definition \@ref(def:exact-definition-merge-node). Variants of Definition \@ref(def:cc-convolutional) have appeared in recent works on higher-order networks [@bunch2020simplicial; @ebli2020simplicial; @hajijcell; @schaub2020random; @schaub2021signal; @roddenberry2021principled; @calmon2022higher; @hajij2021simplicial; @roddenberry2021signal; @yang2023convolutional].

```{definition, cc-convolutional, name="CC-convolutional merge node"}
Let a $\mathcal{X}$ be a CC. Moreover, let $G_1\colon\mathcal{C}^{i_1}(\mathcal{X}) \to\mathcal{C}^j(\mathcal{X})$ and $G_2\colon\mathcal{C}^{i_2}(\mathcal{X})\to\mathcal{C}^j(\mathcal{X})$ be two cochain maps. Given a cochain vector $(\mathbf{H}_{i_1},\mathbf{H}_{i_2}) \in \mathcal{C}^{i_1}\times \mathcal{C}^{i_2}$, a *CC-convolutional merge node* $\mathcal{M}^{conv}_{\mathbf{G};\mathbf{W}} \colon\mathcal{C}^{i_1} \times \mathcal{C}^{i_2} \to \mathcal{C}^j$ is defined as
\begin{equation}
\begin{aligned}
\mathcal{M}^{conv}_{\mathbf{G};\mathbf{W}}
(\mathbf{H}_{i_1},\mathbf{H}_{i_2}) &=
\beta\left( \mathcal{F}^{conv}_{G_1;W_1}(\mathbf{H}_{i_1})
+ \mathcal{F}^{conv}_{G_2;W_2}(\mathbf{H}_{i_2})  \right)\\
&= \beta ( G_1 \mathbf{H}_{i_1} W_1  +  G_2 \mathbf{H}_{i_2} W_2 ),\\
\end{aligned}
\end{equation}
where
$\mathbf{G}=(G_1, G_2)$, $\mathbf{W}=(W_1, W_2)$ is a tuple of trainable parameters, and $\beta$ is an activation function.
```

In practice, the matrix representation of the cochain map $G$ in Definition \@ref(def:cc-conv-pushforward) might require problem-specific normalization during training. For various types of normalization in the context of higher-order convolutional operators, we refer the reader to [@kipf2016semi; @bunch2020simplicial; @schaub2020random].

We have used the notation $\mbox{CCNN}_{\mathbf{G}}$ for a tensor diagram and its corresponding CCNN. Our notation indicates that the CCNN is composed of elementary tensor operations based on a sequence $\mathbf{G}= (G_i)_{i=1}^l$  of cochain maps defined on the underlying CC. When the elementary tensor operations that make up the CCNN are parameterized by a sequence $\mathbf{W}= (W_i)_{i=1}^k$ of trainable parameters, we denote the CCNN and its tensor diagram representation by $\mbox{CCNN}_{\mathbf{G};\mathbf{W}}$.

## Combinatorial complex attention neural networks
