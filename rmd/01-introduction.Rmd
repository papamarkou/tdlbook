\mainmatter

# (PART\*) Part I: Foundations {-}

# Introduction

In recent years, there has been an exponential growth in the amount of data available for computational analysis, including scientific data as well as common data types such as text, images, and audio. This abundance of data has empowered various fields, including physics, chemistry, computational social sciences, and biology, to make significant progress using machine learning techniques, primarily deep neural networks. As deep neural networks can effectively summarize and extract patterns from large data sets, they are suitable for many complex tasks. Initially, deep neural networks have been developed to learn from data supported on regular (Euclidean) domains,
such as grids in images, sequences of text and time-series. These models, including convolutional neural networks (CNNs) [@lecun1998; @krizhevsky2012; @simonyan2014], recurrent neural networks (RNNs) [@bahdanau2014, @sutskever2014] and transformers [@vaswani2017], have proven highly effective in processing such Euclidean data [@goodfellow2016], resulting in unprecedented performance in various applications, most recently in chat-bots (e.g., ChatGPT [@adesso2023]) and text-controlled image synthesis [@rombach2022].

However, scientific data in various fields are often structured differently and are not supported on regular Euclidean domains. As a result, adapting deep neural networks to process this type of data has been a challenge. Against this backdrop, geometric deep learning (GDL) [@wu2020comprehensive; @zhou2020graph; @bronstein2021geometric] has emerged as an extension of deep learning models to non-Euclidean domains. To achieve this, GDL restricts the performed computations via principles of geometric regularity, such as symmetries, invariances, and equivariances. The GDL perspective allows for appropriate inductive biases to be imposed when working with arbitrary data domains, including sets [@qi2017pointnet; @rempe2020caspr; @deng2018ppfnet; @zhao20223dpointcaps; @huang2022multiway], grids [@boscaini2015learning, @masci2015geodesic, @boscaini2016learning, @kokkinos2012intrinsic; @shuman2016vertex; @wu20153d; @monti2017geometric], manifolds [@boscaini2015learning; @masci2015geodesic; @boscaini2016learning; @kokkinos2012intrinsic; @shuman2016vertex; @wu20153d; @monti2017geometric], and graphs [@scarselli2008graph; @gallicchio2010graph; @zhou2020graph; @wu2020comprehensive; @boscaini2016learning; @monti2017geometric; @bronstein2017geometric; @kipf2016semi]. Graphs, in particular, have garnered interest due to their applicability in numerous scientific studies and their ability to generalize conventional grids. Accordingly, the development of graph neural networks (GNNs) [@bronstein2017geometric; @kipf2016semi] has remarkably enhanced our ability to model and analyze several types of data in which graphs naturally appear.

Despite the success of GDL and GNNs, seeing graphs through a purely geometric viewpoint yields a solely local abstraction and falls short of capturing non-local properties and dependencies in data. *Topological data*, including interactions of edges (in graphs), triangles (in meshes) or cliques, arise naturally in an array of novel applications in complex physical systems [@battiston2021physics; @lambiotte2019networks], traffic forecasting [@jiang2022graph], social influence [@zhu2018social], protein interaction [@murgas2022hypergraph], molecular design [@schiff2020characterizing], visual enhancement [@efthymiou2021graph], recommendation systems [@la2022music], and epidemiology [@deng2020cola]. To natively and effectively model such data, we are bound to go beyond graphs and consider qualitative spatial properties remaining unchanged under some geometric transformations. In other words, we need to consider the *topology of data* [@carlsson2009topology] to formulate neural network architectures capable of extracting semantic meaning from complex data.

One approach to extract more global information from data is to go beyond graph-based abstractions and consider extensions of graphs, such as simplicial complexes, cell complexes, and hypergraphs, generalizing most data domains encountered in scientific computations [@bick2021higher; @battiston2020networks; @benson2021higher; @torres2021and]. The development of machine learning models to learn from data supported on these topological domains [@feng2019hypergraph; @bunch2020simplicial; @roddenberry2021signal; @schaub2020random; @billings2019simplex2vec; @hacker2020k; @hajijcell; @ebli2020simplicial; @schaub2021signal; @roddenberry2021principled; @giusti2022cell; @yang2023convolutional] is a rapidly growing new frontier, to which we refer hereafter as *topological deep learning (TDL)*. TDL intertwines several research areas, including topological data analysis (TDA) [@edelsbrunner2010computational; @carlsson2009topology; @dey22; @love2023topological; @ghrist2014elementary], topological signal processing [@schaub2018denoising; @yang2021finite; @schaub2022signal; @roddenberry2021signal; @barbarossa2020topological; @robinson2014topological; @sardellitti2022topological], network science [@skardal2021higher; @lambiotte2019networks; @barabasi2013network; @battiston2020networks; @bick2021higher; @bianconi2021higher; @benson2016higher; @de2016physics; @bao2022impact; @oballe2021bayesian}, and geometric deep learning [@zhang2020deep; @cao2020comprehensive; @fey2019fast; @loukas2019graph; @battaglia2018relational; @morris2019weisfeiler; @battaglia2016interaction].

```{r, echo=FALSE, fig.align="center", fig.cap="A graphical abstract that visualizes our main contributions. (a): Different mathematical structures can be used to represent relations between abstract entities. Sets have entities with no connections, graphs encode binary relations between vertices, simplicial and cell complexes model hierarchical higher-order relations, and hypergraphs accommodate arbitrary set-type relations with no hierarchy. We introduce combinatorial complexes (CCs), which generalize graphs, simplicial and cell complexes, and hypergraphs. CCs are equipped with set-type relations as well as with a hierarchy of these relations. (b): By utilizing the hierarchical and topological structure of CCs, we introduce the push-forward operation, a fundamental building block for higher-order message-passing protocols and for (un)pooling operations on CCs. Our push-forward operations on CCs enable us to construct combinatorial complex neural networks (CCNNs), which provide a general conceptual framework for topological deep learning on higher-order domains."}
knitr::include_graphics('figures/main_figure.png', dpi=NA)
```
