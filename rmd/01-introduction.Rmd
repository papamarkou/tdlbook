\mainmatter

# (PART\*) Part I: Foundations {-}

# Introduction

In recent years, there has been an exponential growth in the amount of data available for computational analysis, including scientific data as well as common data types such as text, images, and audio. This abundance of data has empowered various fields, including physics, chemistry, computational social sciences, and biology, to make significant progress using machine learning techniques, primarily deep neural networks. As deep neural networks can effectively summarize and extract patterns from large data sets, they are suitable for many complex tasks. Initially, deep neural networks have been developed to learn from data supported on regular (Euclidean) domains,
such as grids in images, sequences of text and time-series. These models, including convolutional neural networks (CNNs) [@lecun1998; @krizhevsky2012; @simonyan2014], recurrent neural networks (RNNs) [@bahdanau2014, @sutskever2014] and transformers [@vaswani2017], have proven highly effective in processing such Euclidean data [@goodfellow2016], resulting in unprecedented performance in various applications, most recently in chat-bots (e.g., ChatGPT [@adesso2023]) and text-controlled image synthesis [@rombach2022].

```{r, echo=FALSE, fig.align="center", fig.cap="A graphical abstract that visualizes our main contributions. (a): Different mathematical structures can be used to represent relations between abstract entities. Sets have entities with no connections, graphs encode binary relations between vertices, simplicial and cell complexes model hierarchical higher-order relations, and hypergraphs accommodate arbitrary set-type relations with no hierarchy. We introduce combinatorial complexes (CCs), which generalize graphs, simplicial and cell complexes, and hypergraphs. CCs are equipped with set-type relations as well as with a hierarchy of these relations. (b): By utilizing the hierarchical and topological structure of CCs, we introduce the push-forward operation, a fundamental building block for higher-order message-passing protocols and for (un)pooling operations on CCs. Our push-forward operations on CCs enable us to construct combinatorial complex neural networks (CCNNs), which provide a general conceptual framework for topological deep learning on higher-order domains."}
knitr::include_graphics('figures/main_figure.png', dpi=NA)
```
